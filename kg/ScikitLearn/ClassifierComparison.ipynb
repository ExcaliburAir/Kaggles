{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模型\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各种分类模型\n",
    "classifiers = [\n",
    "    # K-近邻算法(KNN)：根据邻居判断类型。\n",
    "    # 如果一个样本在特征空间中有K个最相似（距离相近）的样本大多数属于一个类别，则该样品也属于这个类别。\n",
    "    # 默认变化是线性的，对异常点脆弱。\n",
    "    KNeighborsClassifier(3), \n",
    "    # SVM=Support Vector Machine 是支持向量。\n",
    "    # SVC=Support Vector Classification就是支持向量机用于分类。\n",
    "    # SVC=Support Vector Regression.就是支持向量机用于回归分析。\n",
    "    # 重点是边缘的支持向量，和，核函数处理非线性。\n",
    "    SVC(probability=True),\n",
    "    # 决策树\n",
    "    # 通过对不同特征节点提问而进行分类。\n",
    "    DecisionTreeClassifier(),\n",
    "    # 随机森林\n",
    "    # 有很多独立的决策树用bargging方式组成的模型融合系统。\n",
    "    # 每棵树随机学习数据的不同特征从而产生一定的独立性。\n",
    "    RandomForestClassifier(),\n",
    "    # 本质是boosting模型融合\n",
    "    # boosting通过上一次训练结果，提升数据权重，训练下一个模型，然后吧所有模型ensamble。\n",
    "    # 理论上任何学习器都可以用于Adaboost。\n",
    "    # 但一般来说，使用最广泛的Adaboost弱学习器是决策树和神经网络。\n",
    "    # 对于决策树，Adaboost分类用了CART分类树，而Adaboost回归用了CART回归树。\n",
    "    AdaBoostClassifier(),\n",
    "    # GBDT\n",
    "    # scikit-learn里没有xgboost，所以GBDT是skelearn里能用最好的boosting。\n",
    "    # 在AdaBoost中，base learner是classfication tree，即f(x) = 1 or -1\n",
    "    # 在GBDT中，base learner是regression tree，f(x)为一个score，可以为-0.3，可以为0.2，可以为1\n",
    "    # 在XGBoost中，base learner是regression tree，f(x)为一个score，可以为-0.3，可以为0.2，可以为1\n",
    "    # XGBoost引入了多种方式防止过拟合。\n",
    "    # XGBoost确实引入多有意思的东西，比如parallization, column block，sampling, dart等\n",
    "    GradientBoostingClassifier(),\n",
    "    # 连续型朴素贝叶斯\n",
    "    # GaussianNB 实现了运用于分类的高斯朴素贝叶斯算法。特征的可能性(即概率)假设为高斯分布。\n",
    "    # 贝叶斯的算法我是接受的，但是特征分布假设不一定是高斯的。\n",
    "    GaussianNB(),\n",
    "    # 线性决策平面：LDA\n",
    "    # 因为他们可以很容易计算得到闭式解(即解析解)，其天生具有多分类的特性，在实践中已经被证明很有效，并且无需调参。\n",
    "    # LDA 和 QDA 都是源于简单的概率模型，这些模型对于每一个类别 k 的相关分布P(X|y=k) 都可以通过贝叶斯定理所获得。\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    # 二次决策平面：QDA\n",
    "    # 但是他们的模型复杂度是不够的，所以还是有很大局限性。\n",
    "    # LDA 和 QDA 都是源于简单的概率模型，这些模型对于每一个类别 k 的相关分布P(X|y=k) 都可以通过贝叶斯定理所获得。\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    # 罗辑回归\n",
    "    # scikit-learn 中 logistic 回归在 LogisticRegression 类中实现了二分类（binary）、一对多分类（one-vs-rest）及多项式 logistic 回归，并带有可选的 L1 和 L2 正则化。\n",
    "    # 虽然名字里有 “回归” 二字，但实际上是解决分类问题的一类线性模型。\n",
    "    # 最小化目标函数loss function。\n",
    "    # 实现了这些优化算法: liblinear， newton-cg， lbfgs， sag 和 saga。\n",
    "    LogisticRegression()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "log_cols = [\"Classifier\", \"Accuracy\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "# 随机划分\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n",
    "\n",
    "X = train[0::, 1::]\n",
    "y = train[0::, 0]\n",
    "\n",
    "acc_dict = {}\n",
    "\n",
    "# 训练并得到结果\n",
    "# 没有做交叉验证等工作，只是个base\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "        # 训练，测试，特征数据，标签数据\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # 开始训练\n",
    "        for clf in classifiers:\n",
    "            name = clf.__class__.__name__ # 模型类名\n",
    "            clf.fit(X_train, y_train) # 一句话就是训练\n",
    "            train_predictions = clf.predict(X_test) # 得到预测结果\n",
    "            acc = accuracy_score(y_test, train_predictions) # 计算预测精度\n",
    "            \n",
    "            # 放到acc_dict里\n",
    "            if name in acc_dict:\n",
    "                acc_dict[name] += acc\n",
    "            else:\n",
    "                acc_dict[name] = acc\n",
    "\n",
    "# 根据结果画图\n",
    "for clf in acc_dict:\n",
    "        acc_dict[clf] = acc_dict[clf] / 10.0\n",
    "        log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n",
    "        log = log.append(log_entry)\n",
    "\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Classifier Accuracy')\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
